import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import itertools
import sys
import torch.nn.functional as F

# --- 0. Setup and Imports ---
# !pip install Pillow # Ensure Pillow is available for image handling

# --- 1. Configuration (Revised) ---
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {DEVICE}")

BATCH_SIZE = 128
N_EPOCHS_SOURCE = 20 # Increased from 10 for stronger baseline
N_EPOCHS_ADAPT = 100 # Increased from 30 for slower, stable convergence
LR = 1e-4             # Base Learning Rate for Source & Discriminator
LR_TARGET_ENCODER = 1e-5 # CRITICAL FIX: 10x lower LR for E_T to prevent negative transfer
LR_TARGET_CLASSIFIER = 1e-4 # New: LR for C_T (from original task description)
LAMBDA_ENTROPY = 0.01       # New: Weight for entropy minimization loss (from original task description)

# --- 2. Data Transformations ---
transform_svhn = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

transform_mnist = transforms.Compose([
    transforms.Resize((32, 32)),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# --- 3. Data Loading ---
try:
    mnist_train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform_mnist)
    mnist_test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform_mnist)
    svhn_train_dataset = datasets.SVHN('./data', split='train', download=True, transform=transform_svhn)
    svhn_test_dataset = datasets.SVHN('./data', split='test', download=True, transform=transform_svhn)

    mnist_train_loader = DataLoader(mnist_train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=2)
    mnist_test_loader = DataLoader(mnist_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)
    svhn_train_loader = DataLoader(svhn_train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=2)
    svhn_test_loader = DataLoader(svhn_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

    print("Data successfully loaded and DataLoaders are defined.")

except Exception as e:
    print(f"Error loading data: {e}")
    sys.exit(1)

# --- 4. Model Architectures (Unchanged) ---

class FeatureExtractor(nn.Module):
    def __init__(self):
        super(FeatureExtractor, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=5, padding=2), nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(32, 48, kernel_size=5, padding=2), nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Dropout2d(0.5)
        )
        self.fc = nn.Sequential(
            nn.Linear(48 * 8 * 8, 512), nn.ReLU(),
            nn.Linear(512, 256), nn.ReLU()
        )

    def forward(self, x):
        x = self.conv(x)
        x = x.view(-1, 48 * 8 * 8)
        x = self.fc(x)
        return x

class Classifier(nn.Module):
    def __init__(self):
        super(Classifier, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(256, 10)
        )

    def forward(self, x):
        return self.fc(x)

class DomainDiscriminator(nn.Module):
    def __init__(self):
        super(DomainDiscriminator, self).__init__()
        self.fc = nn.Sequential(
            nn.utils.spectral_norm(nn.Linear(256, 512)), nn.ReLU(),
            nn.utils.spectral_norm(nn.Linear(512, 1))
        )

    def forward(self, x):
        return self.fc(x)

# --- 5. Initialization ---
E_S = FeatureExtractor().to(DEVICE)
C = Classifier().to(DEVICE)
D = DomainDiscriminator().to(DEVICE)
E_T = FeatureExtractor().to(DEVICE)
C_T = Classifier().to(DEVICE) # C_T will be used for entropy loss

# --- 6. Helper Function ---
def evaluate_model(encoder, classifier, data_loader, name="Test"):
    """Evaluates the classification accuracy on a dataset."""
    encoder.eval()
    classifier.eval()
    total_correct = 0
    total_count = 0
    with torch.no_grad():
        for data, label in data_loader:
            data, label = data.to(DEVICE), label.to(DEVICE)

            features = encoder(data)
            output = classifier(features)

            pred = output.argmax(dim=1)
            total_correct += (pred == label).sum().item()
            total_count += label.size(0)

    accuracy = 100. * total_correct / total_count
    print(f'*** {name} Accuracy: {accuracy:.2f}% ({total_correct}/{total_count}) ***')
    return accuracy

# ==============================================================================
# STAGE 1: Source Pre-training (Train E_S and C on labeled MNIST)
# ==============================================================================
print(f"\n--- STAGE 1: Source Pre-training on MNIST ({N_EPOCHS_SOURCE} Epochs) ---")
optimizer_source = optim.Adam(itertools.chain(E_S.parameters(), C.parameters()), lr=LR)
criterion_class = nn.CrossEntropyLoss()

for epoch in range(1, N_EPOCHS_SOURCE + 1):
    E_S.train()
    C.train()

    for i, (data, label) in enumerate(mnist_train_loader):
        data, label = data.to(DEVICE), label.to(DEVICE)

        optimizer_source.zero_grad()
        features = E_S(data)
        output = C(features)
        loss = criterion_class(output, label)
        loss.backward()
        optimizer_source.step()

        if i % 200 == 0:
            sys.stdout.write(f'Source Epoch [{epoch}/{N_EPOCHS_SOURCE}], Batch [{i}/{len(mnist_train_loader)}], Loss: {loss.item():.4f}\r')
            sys.stdout.flush()
    print(f'Source Epoch [{epoch}/{N_EPOCHS_SOURCE}] completed. Final Batch Loss: {loss.item():.4f}')


# Evaluation after pre-training
print("\n--- Evaluation of Pre-trained Model (Source E_S) ---")
evaluate_model(E_S, C, mnist_test_loader, name="MNIST Test (Source)")
evaluate_model(E_S, C, svhn_test_loader, name="SVHN Test (Initial Target - Domain Shift)")


# --- Initialization and Freezing ---
E_T.load_state_dict(E_S.state_dict())
C_T.load_state_dict(C.state_dict()) # C_T is now a copy of the source classifier C
E_S.eval()
C.eval()
for param in E_S.parameters(): param.requires_grad = False
for param in C.parameters(): param.requires_grad = False # Freeze source classifier


# ==============================================================================
# STAGE 2: Adversarial Adaptation (Train E_T and D) - CRITICAL FIX APPLIED
# ==============================================================================
print(f"\n--- STAGE 2: Adversarial Adaptation (MNIST->SVHN) ({N_EPOCHS_ADAPT} Epochs) ---")
optimizer_D = optim.Adam(D.parameters(), lr=LR)
# *** FIX APPLIED HERE: E_T's LR is 10x lower to prevent negative transfer ***
optimizer_E_T = optim.Adam(E_T.parameters(), lr=LR_TARGET_ENCODER)
# *** C_T is not explicitly trained with its own optimizer in this ADDA variant,
# *** but it is used for entropy minimization with E_T
# optimizer_C_T = optim.Adam(C_T.parameters(), lr=LR_TARGET_CLASSIFIER) # This optimizer is not used in this fixed version
criterion_domain = nn.BCEWithLogitsLoss()

source_iter = iter(mnist_train_loader)
target_iter = iter(svhn_train_loader)
len_loader = max(len(mnist_train_loader), len(svhn_train_loader))

for epoch in range(1, N_EPOCHS_ADAPT + 1):
    E_T.train()
    D.train()

    for step in range(len_loader):
        # Reset iterators if depleted
        try:
            x_s, _ = next(source_iter)
        except StopIteration:
            source_iter = iter(mnist_train_loader)
            x_s, _ = next(source_iter)

        try:
            x_t, _ = next(target_iter)
        except StopIteration:
            target_iter = iter(svhn_train_loader)
            x_t, _ = next(target_iter)

        x_s, x_t = x_s.to(DEVICE), x_t.to(DEVICE)

        # --- 1. Train Domain Discriminator (D) ---
        optimizer_D.zero_grad()

        # Source features (Label 0 - Source for D, using smoothed labels)
        features_s = E_S(x_s).detach() # E_S is frozen
        d_out_s = D(features_s).squeeze(1)
        loss_D_s = criterion_domain(d_out_s, torch.full_like(d_out_s, 0.1)) # Smooth label 0 for source

        # Target features (Label 1 - Target for D, using smoothed labels)
        features_t = E_T(x_t).detach() # Detach E_T to only train D
        d_out_t = D(features_t).squeeze(1)
        loss_D_t = criterion_domain(d_out_t, torch.full_like(d_out_t, 0.9)) # Smooth label 1 for target

        loss_D = loss_D_s + loss_D_t
        loss_D.backward() # Backpropagate D's loss
        optimizer_D.step()

        # --- 2. Train Target Feature Extractor (E_T) - Adversarial Step + Entropy Minimization ---
        optimizer_E_T.zero_grad()

        # Adversarial loss: E_T tries to fool D (make D classify target as source)
        features_t_for_E_T = E_T(x_t)
        d_out_t_adv = D(features_t_for_E_T).squeeze(1)
        loss_E_T_adv = criterion_domain(d_out_t_adv, torch.full_like(d_out_t_adv, 0.1)) # E_T wants D to output 0.1 for target (source label)

        # Entropy Minimization loss: E_T tries to make C_T confident on target predictions
        # C_T is a fixed copy of the source classifier C during this phase
        predictions_t_for_entropy = C_T(features_t_for_E_T)
        softmax_predictions_t_for_entropy = F.softmax(predictions_t_for_entropy, dim=1)
        # Minimize negative entropy (i.e., maximize confidence)
        entropy_loss = (-softmax_predictions_t_for_entropy * torch.log(softmax_predictions_t_for_entropy + 1e-10)).sum(dim=1).mean()

        loss_E_T = loss_E_T_adv + LAMBDA_ENTROPY * entropy_loss
        loss_E_T.backward() # Backpropagate E_T's loss
        optimizer_E_T.step()

    # Log and Evaluate
    if epoch % 10 == 0:
        print(f'\n--- Epoch {epoch}/{N_EPOCHS_ADAPT} Evaluation ---')
        print(f'Adapt Epoch Loss: D: {loss_D.item():.4f}, E_T (adv+ent): {loss_E_T.item():.4f}')
        # Evaluate using the adapted target encoder (E_T) and the frozen source classifier (C)
        evaluate_model(E_T, C, svhn_test_loader, name=f"SVHN Test (ADDA)")
    else:
        sys.stdout.write(f'Adapt Epoch [{epoch}/{N_EPOCHS_ADAPT}], D Loss: {loss_D.item():.4f}, E_T (adv+ent) Loss: {loss_E_T.item():.4f}\r')
        sys.stdout.flush()

# --- Final Evaluation ---
print("\n--- Final ADDA Evaluation on SVHN Target Domain ---")
evaluate_model(E_T, C, svhn_test_loader, name="SVHN Test (ADDA Final)")
